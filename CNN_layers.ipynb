{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this Jupyter Notebook we will code from scratch a CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Imports ###\n",
    "\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "from tqdm import tqdm\n",
    "\n",
    "np.random.seed(0)  # For reproductibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I - Mother class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class layer :\n",
    "\n",
    "    def __init__(self) :\n",
    "\n",
    "        self.input = None\n",
    "        self.output = None\n",
    "\n",
    "    def forward(self, input) :\n",
    "        pass\n",
    "\n",
    "    def backward(self, grad, eta) :    # eta : learning rate\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "II - Dense layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dense_layer(layer) :\n",
    "\n",
    "    def __init__(self,\n",
    "                 nb_inputs, \n",
    "                 nb_neurones, \n",
    "                 weights_initializer = \"xavier_norm\", \n",
    "                 biases_initializer = \"xavier_norm\",\n",
    "                 w_std_dev_init = 1,\n",
    "                 w_mean_init = 0,\n",
    "                 w_low_uni_init = -1,\n",
    "                 w_high_uni_init = 1,\n",
    "                 b_std_dev_init = 1,\n",
    "                 b_mean_init = 0,\n",
    "                 b_low_uni_init = -1,\n",
    "                 b_high_uni_init = 1,\n",
    "                 optimizer = \"GD\",\n",
    "                 moment_gd_param = 0.2\n",
    "                 ) :\n",
    "        \n",
    "        self.nb_neurones = nb_neurones\n",
    "\n",
    "        if optimizer not in [\"GD\", \"moment_GD\", \"SGD\"] :\n",
    "\n",
    "            raise TypeError(\"The optimizer should be in ['GD', 'moment_GD', 'SGD']\")\n",
    "        \n",
    "        self.optimizer = optimizer\n",
    "\n",
    "        if optimizer == \"moment_GD\" :\n",
    "\n",
    "            self.moment_gd_param = moment_gd_param\n",
    "\n",
    "        if weights_initializer not in [\"zero\", \"rndu\", \"rndn\", \"xavier_uni\", \"xavier_norm\", \"He\"] :\n",
    "\n",
    "            raise TypeError(\"The initializers should be in ['zero', 'rndu', 'rndn', 'xavier_uni', 'xavier_norm', 'He']\")\n",
    "\n",
    "        if biases_initializer not in [\"zero\", \"rndu\", \"rndn\", \"xavier_uni\", \"xavier_norm\", \"He\"] :\n",
    "\n",
    "            raise TypeError(\"The initializers should be in ['zero', 'rndu', 'rndn', 'xavier_uni', 'xavier_norm', 'He']\")\n",
    "        \n",
    "        if weights_initializer == \"zero\" :\n",
    "            self.weights = np.zeros((nb_neurones, nb_inputs))\n",
    "\n",
    "        if weights_initializer == \"rndu\" :\n",
    "            self.weights = np.random.uniform(low = w_low_uni_init, high = w_high_uni_init, size = (nb_neurones, nb_inputs))\n",
    "\n",
    "        if weights_initializer == \"rndn\" :\n",
    "            self.weights = w_mean_init + w_std_dev_init*np.random.standard_normal(size = (nb_neurones, nb_inputs))\n",
    "\n",
    "        if weights_initializer == \"xavier_uni\" :\n",
    "            r = np.sqrt(6/(nb_neurones + nb_inputs))\n",
    "            self.weights = np.random.uniform(low = -r, high = r, size = (nb_neurones, nb_inputs))\n",
    "\n",
    "        if weights_initializer == \"xavier_norm\" :\n",
    "            sigma = np.sqrt(6/(nb_neurones + nb_inputs))\n",
    "            self.weights = sigma*np.random.standard_normal(size = (nb_neurones, nb_inputs))\n",
    "\n",
    "        if weights_initializer == \"He\" :\n",
    "            sigma = np.sqrt(2/nb_inputs)\n",
    "            self.weights = sigma*np.random.standard_normal(size = (nb_neurones, nb_inputs))\n",
    "        \n",
    "        if biases_initializer == \"zero\" :\n",
    "            self.biases = np.zeros(nb_neurones)\n",
    "\n",
    "        if biases_initializer == \"rndu\" :\n",
    "            self.biases = np.random.uniform(low = b_low_uni_init, high = b_high_uni_init, size = (nb_neurones, nb_inputs))\n",
    "\n",
    "        if biases_initializer == \"rndn\" :\n",
    "            self.biases = b_mean_init + b_std_dev_init*np.random.standard_normal(size = nb_neurones)\n",
    "\n",
    "        if biases_initializer == \"xavier_uni\" :\n",
    "            r = np.sqrt(6/(nb_neurones + nb_inputs))\n",
    "            self.biases = np.random.uniform(low = -r, high = r, size = nb_neurones)\n",
    "\n",
    "        if biases_initializer == \"xavier_norm\" :\n",
    "            sigma = np.sqrt(6/(nb_neurones + nb_inputs))\n",
    "            self.biases = sigma*np.random.standard_normal( size = nb_neurones)\n",
    "\n",
    "        if biases_initializer == \"He\" :\n",
    "            sigma = np.sqrt(2/nb_inputs)\n",
    "            self.biases = sigma*np.random.standard_normal( size = nb_neurones)\n",
    "\n",
    "\n",
    "    def forward(self, input) :\n",
    "\n",
    "        self.input = input\n",
    "\n",
    "        return np.dot(self.weights, self.input) + self.biases\n",
    "\n",
    "    def backward(self, grad, eta) :\n",
    "\n",
    "        weights_grad = np.dot(np.reshape(grad,(grad.size,1)), np.reshape(self.input,(1,self.input.size)))\n",
    "\n",
    "        if self.optimizer == \"GD\" : \n",
    "\n",
    "            self.weights -= eta*weights_grad      # weights update\n",
    "            self.biases -= eta*grad             # biases update\n",
    "\n",
    "        if self.optimizer == \"moment_GD\" :\n",
    "            \n",
    "            self.momentum = []    #Momentum for the weights and the biases in this order\n",
    "\n",
    "            gamma = self.moment_gd_param\n",
    "\n",
    "            if len(self.momentum) == 0 :\n",
    "                self.momentum.append(weights_grad)\n",
    "                self.momentum.append(grad)\n",
    "            \n",
    "            else :\n",
    "                self.momentum.append(gamma*self.momentum[-2] + (1-gamma)*weights_grad)\n",
    "                self.momentum.append(gamma*self.momentum[-2] + (1-gamma)*grad)\n",
    "                self.momentum = self.momentum[-2:]\n",
    "            \n",
    "            self.weights -= eta*self.momentum[-2]      # weights update\n",
    "            self.biases -= eta*self.momentum[-1]       # biases update\n",
    "    \n",
    "        return np.dot(self.weights.T, grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "III - Convolutionnal layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class convolutionnal_layer(layer) :\n",
    "\n",
    "    def __init__(self,\n",
    "                 input_shape,\n",
    "                 kernel_size,\n",
    "                 depth,\n",
    "                 optimizer = \"GD\",\n",
    "                 moment_gd_param = 0.2\n",
    "                 ) :\n",
    "\n",
    "\n",
    "        # We first deal with the dimension of our images, kernes and features\n",
    "\n",
    "        input_depth, input_height, input_width =  input_shape\n",
    "\n",
    "        self.depth = depth\n",
    "        self.input_shape = input_shape\n",
    "        self.input_depth = input_depth\n",
    "        self.output_shape = (depth, input_height - kernel_size + 1, input_width - kernel_size + 1)\n",
    "        self.kernels_shape = (depth, input_depth, kernel_size, kernel_size)\n",
    "\n",
    "        if optimizer not in [\"GD\", \"moment_GD\", \"SGD\"] :\n",
    "\n",
    "            raise TypeError(\"The optimizer should be in ['GD', 'moment_GD', 'SGD']\")\n",
    "        \n",
    "        self.optimizer = optimizer\n",
    "\n",
    "        if optimizer == \"moment_GD\" :\n",
    "\n",
    "            self.moment_gd_param = moment_gd_param\n",
    "\n",
    "        # He initialization for ReLU activation\n",
    "        \n",
    "        input_size = np.prod(np.array(input_shape))\n",
    "        sigma = np.sqrt(2/input_size)\n",
    "        \n",
    "        self.kernels = sigma*np.random.standard_normal( size = self.kernels_shape)\n",
    "        self.biases = sigma*np.random.standard_normal(size = self.output_shape)\n",
    "\n",
    "    def forward(self, input) :\n",
    "\n",
    "        self.input = input\n",
    "        self.output = np.copy(self.biases)\n",
    "        \n",
    "        for i in range(self.depth) :\n",
    "            for j in range(self.input_depth) :\n",
    "            \n",
    "                self.output[i] += signal.correlate2d(self.input[j], self.kernels[i,j], 'valid')\n",
    "                \n",
    "        return self.output\n",
    "\n",
    "    def backward(self, grad, eta) :\n",
    "\n",
    "        kernels_grad = np.zeros(self.kernels_shape)\n",
    "        input_grad = np.zeros(self.input_shape)\n",
    "\n",
    "        for i in range(self.depth) :\n",
    "            for j in range(self.input_depth) :\n",
    "\n",
    "                kernels_grad[i,j] = signal.correlate2d(self.input[j], grad[i], 'valid')\n",
    "                input_grad[j] +=  signal.convolve2d(grad[i], self.kernels[i,j], 'full')\n",
    "\n",
    "        if self.optimizer == \"GD\" :\n",
    "\n",
    "            self.kernels -= eta * kernels_grad\n",
    "            self.biases -= eta * grad\n",
    "\n",
    "        if self.optimizer == \"moment_GD\" :\n",
    "\n",
    "            self.momentum = []    #Momentum for the weights and the biases in this order\n",
    "\n",
    "            gamma = self.moment_gd_param\n",
    "\n",
    "            if len(self.momentum) == 0 :\n",
    "                self.momentum.append(kernels_grad)\n",
    "                self.momentum.append(grad)\n",
    "            \n",
    "            else :\n",
    "                self.momentum.append(gamma*self.momentum[-2] + (1-gamma)*kernels_grad)\n",
    "                self.momentum.append(gamma*self.momentum[-2] + (1-gamma)*grad)\n",
    "                self.momentum = self.momentum[-2:]\n",
    "            \n",
    "            self.kernels -= eta*self.momentum[-2]      # kernels update\n",
    "            self.biases -= eta*self.momentum[-1]       # biases update\n",
    "\n",
    "        return input_grad\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "III - Activation layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sigmoid_activation_layer(layer) :\n",
    "\n",
    "    def __init__(self) :\n",
    "\n",
    "        self.activ_func = lambda x : 1/(1 + np.exp(-x))\n",
    "        self.derivative = lambda x : 1/(1 + np.exp(-x))*(1 - 1/(1 + np.exp(-x)))\n",
    "\n",
    "    def forward(self, input) :\n",
    "\n",
    "        self.input = input\n",
    "        return self.activ_func(self.input)\n",
    "\n",
    "    def backward(self, grad, eta) :\n",
    "\n",
    "        return np.multiply(grad, self.derivative(self.input))\n",
    "\n",
    "\n",
    "        \n",
    "        return np.dot(grad, self.output)*self.output + np.multiply(grad,self.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class softmax_activation_layer(layer) : \n",
    "\n",
    "    def __init__(self,pr = False) :\n",
    "\n",
    "        self.pr = pr\n",
    "\n",
    "    def forward(self,input) : \n",
    "\n",
    "        if self.pr :\n",
    "            print(np.exp(input))\n",
    "            print(np.sum(np.exp(input)))\n",
    "              \n",
    "        self.output = np.exp(input-np.max(input))/np.sum(np.exp(input-np.max(input)))\n",
    "        \n",
    "        \n",
    "        return np.clip(self.output, 10e-7, 1 - 10e-7)\n",
    "\n",
    "    def backward(self,grad,eta) : \n",
    "        \n",
    "        return np.dot(grad, self.output)*self.output + np.multiply(grad,self.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class tanh_activation_layer(layer) : \n",
    "\n",
    "    def __init__(self) :\n",
    "\n",
    "        self.activ_func = lambda x : np.tanh(x)\n",
    "        self.derivative = lambda x : 1 - np.tanh(x)**2\n",
    "\n",
    "    def forward(self, input) :\n",
    "\n",
    "        self.input = input\n",
    "        return self.activ_func(self.input)\n",
    "\n",
    "    def backward(self, grad, eta) :\n",
    "\n",
    "        return np.multiply(grad, self.derivative(self.input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU_activation_layer(layer) :\n",
    "\n",
    "    def __init__(self) :\n",
    "\n",
    "        self.activ_func = lambda x : np.maximum(0,x)\n",
    "\n",
    "        def relu_derivative(x) :\n",
    "\n",
    "            x[x<=0] = 0\n",
    "            x[x>0] = 1 \n",
    "\n",
    "            return x\n",
    "\n",
    "        self.derivative = relu_derivative\n",
    "\n",
    "    def forward(self, input) :\n",
    "\n",
    "        self.input = input\n",
    "        return self.activ_func(self.input)\n",
    "\n",
    "    def backward(self, grad, eta) :\n",
    "\n",
    "        return np.multiply(grad, self.derivative(self.input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class leakyReLU_activation_layer(layer) : \n",
    "\n",
    "    def __init__(self,coeff = 0.2) :\n",
    "\n",
    "        self.activ_func = lambda x : np.maximum(0,x) + coeff*np.minimum(0,x)\n",
    "\n",
    "        def leaky_relu_derivative(x) :\n",
    "\n",
    "            x[x<=0] = coeff\n",
    "            x[x>0] = 1 \n",
    "\n",
    "            return x\n",
    "\n",
    "        self.derivative = leaky_relu_derivative\n",
    "\n",
    "    def forward(self, input) :\n",
    "\n",
    "        self.input = input\n",
    "        return self.activ_func(self.input)\n",
    "\n",
    "    def backward(self, grad, eta) :\n",
    "\n",
    "        return np.multiply(grad, self.derivative(self.input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class special_tanh_activation_layer1D(layer) : \n",
    "\n",
    "    def __init__(self,epsilon) :\n",
    "\n",
    "        self.epsilon = epsilon\n",
    "        self.activ_func = lambda x : np.tanh(x)\n",
    "        self.derivative = lambda x : 1 - np.tanh(x)**2\n",
    "\n",
    "    def forward(self, input) :\n",
    "\n",
    "        self.input = input\n",
    "        return 709*self.activ_func(self.epsilon + self.input/np.mean(self.input))\n",
    "\n",
    "    def backward(self, grad, eta) :\n",
    "        \n",
    "        mu = np.mean(self.input)\n",
    "        print(f\"softmax grad mean : \")\n",
    "        output = np.zeros(grad.shape)\n",
    "\n",
    "        for i in range(grad.size) :\n",
    "            \n",
    "            delt = 709*(grad.size*mu**2)**(-1)*self.derivative(self.epsilon + self.input/mu)\n",
    "            \n",
    "            for j in range(grad.size) :\n",
    "\n",
    "                if i != j :\n",
    "\n",
    "                    delt[i] *= -self.input[j]\n",
    "\n",
    "                else :\n",
    "\n",
    "                    delt[i] *= grad.size*mu - self.input[j]\n",
    "\n",
    "            output[i][0] += np.dot(np.transpose(np.reshape(grad, (grad.size,1))), delt)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IV - Reshape layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class reshape_layer(layer) :\n",
    "\n",
    "    def __init__(self, input_shape, output_shape) :\n",
    "\n",
    "        self.input_shape = input_shape\n",
    "        self.output_shape = output_shape\n",
    "\n",
    "    def forward(self, input) :\n",
    "\n",
    "        return np.reshape(input, self.output_shape)\n",
    "\n",
    "    def backward(self, grad, eta) :\n",
    "\n",
    "        return np.reshape(grad, self.input_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "V - Pooling layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class avg_pool_layer(layer) :\n",
    "\n",
    "    def __init__(self, input_shape, kernel_size) :\n",
    "\n",
    "\n",
    "        # We first deal with the dimension of our images, kernes and features\n",
    "\n",
    "        input_depth, input_height, input_width =  input_shape\n",
    "\n",
    "        self.input_shape = input_shape\n",
    "        self.input_depth = input_depth\n",
    "        self.output_shape = (input_depth, input_height - kernel_size + 1, input_width - kernel_size + 1)\n",
    "\n",
    "        # We iniatlize the kernel \n",
    "\n",
    "        self.kernels = kernel_size**(-1)*np.ones((input_depth,kernel_size,kernel_size))\n",
    "\n",
    "    def forward(self, input) :\n",
    "\n",
    "        self.input = input\n",
    "        self.output = np.zeros(self.output_shape)\n",
    "\n",
    "        for i in range(self.input_depth) :\n",
    "\n",
    "            self.output[i] += signal.correlate2d(self.input[i], self.kernels[i], 'valid')\n",
    "\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, grad, eta) :\n",
    "\n",
    "        input_grad = np.zeros(self.input_shape)\n",
    "\n",
    "        for i in range(self.input_depth) :\n",
    "\n",
    "            input_grad[i] +=  signal.convolve2d(grad[i], self.kernels[i], 'full')\n",
    "\n",
    "        return input_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class max_pool_layer(layer):\n",
    "\n",
    "  def __init__(self, pr = False) : \n",
    "    self.pr = pr\n",
    "  \n",
    "  def iterate_regions(self, image):\n",
    "    '''\n",
    "    Generates non-overlapping 2x2 image regions to pool over.\n",
    "    - image is a 2d numpy array\n",
    "    '''\n",
    "    _, height, width, = image.shape\n",
    "    new_h = height // 2\n",
    "    new_w = width // 2\n",
    "\n",
    "    for i in range(new_h-1):\n",
    "      for j in range(new_w-1):\n",
    "\n",
    "        im_region = image[(i * 2):(i * 2 + 2), (j * 2):(j * 2 + 2)]\n",
    "\n",
    "        yield im_region, i, j\n",
    "\n",
    "  def forward(self, input):\n",
    "    \n",
    "    nb_kernel, h, w = input.shape\n",
    "    self.input = np.reshape(input, (h, w, nb_kernel))\n",
    "\n",
    "    output = np.zeros((h // 2, w // 2, nb_kernel))\n",
    "    \n",
    "    for im_region, i, j in self.iterate_regions(self.input):\n",
    "      if j >= w//2 :\n",
    "        continue\n",
    "      if i >= h//2:\n",
    "        continue\n",
    "      if self.pr :\n",
    "        print(i,j)\n",
    "      output[i, j] = np.amax(im_region, axis=(0, 1))\n",
    "\n",
    "    return np.reshape(output, (nb_kernel, h // 2, w // 2))\n",
    "\n",
    "  def backward(self, grad, eta):\n",
    "    '''\n",
    "    Performs a backward pass of the maxpool layer.\n",
    "    Returns the loss gradient for this layer's inputs.\n",
    "    - d_L_d_out is the loss gradient for this layer's outputs.\n",
    "    '''\n",
    "    d_L_d_input = np.zeros(self.input.shape)\n",
    "    self.d_L_d_out = np.reshape(grad, (grad.shape[2], grad.shape[1], grad.shape[0]))\n",
    "\n",
    "    for im_region, i, j in self.iterate_regions(self.input):\n",
    "      h, w, f = im_region.shape\n",
    "      amax = np.amax(im_region, axis=(0, 1))\n",
    "\n",
    "      for i2 in range(h):\n",
    "        for j2 in range(w):\n",
    "          for f2 in range(f):\n",
    "            # If this pixel was the max value, copy the gradient to it.\n",
    "            if im_region[i2, j2, f2] == amax[f2]:\n",
    "              d_L_d_input[i * 2 + i2, j * 2 + j2, f2] = self.d_L_d_out[i, j, f2]\n",
    "\n",
    "    return np.reshape(d_L_d_input, (d_L_d_input.shape[2], d_L_d_input.shape[1], d_L_d_input.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VI- Loss layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cross_entropy : \n",
    "\n",
    "    def __init__(self, y_pred, y_true, nb_labels = 10) : \n",
    "\n",
    "        self.y_pred = np.clip(y_pred, 10e-7, 1 - 10e-7)\n",
    "        self.y_true = y_true\n",
    "        self.nb_labels = nb_labels\n",
    "\n",
    "    def compute(self) :\n",
    "\n",
    "        error = -np.log(np.dot(self.y_pred.T, self.y_true)) - np.log(np.dot((1-self.y_pred).T, 1-self.y_true))\n",
    "                \n",
    "        return error[0]\n",
    "    \n",
    "    def grad(self) : \n",
    "\n",
    "        grad = -np.reshape(self.y_true,(self.nb_labels,))/self.y_pred  -np.reshape(1-self.y_true,(self.nb_labels,))/(1-self.y_pred)\n",
    "\n",
    "        return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VII - Normalizing layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class normalize_layer1D(layer) : \n",
    "\n",
    "    def __init__(self, input_shape, epsilon = 1e-5) :\n",
    "        \n",
    "        self.coeff = (np.sqrt(2/input_shape))*np.random.standard_normal(size = 1)\n",
    "        self.biases = (np.sqrt(2/input_shape))*np.random.standard_normal(size = (input_shape))\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "\n",
    "    def forward(self,input) :\n",
    "        \n",
    "        self.mean = np.mean(input)\n",
    "        self.stdev = np.std(input)\n",
    "        self.stand_input = (input - self.mean)/(self.stdev + self.epsilon)\n",
    "         \n",
    "        return self.coeff*self.stand_input + self.biases\n",
    "\n",
    "    def backward(self,grad,eta) : \n",
    "\n",
    "        # grad = np.reshape(grad, (grad.size,1))  # Just in case\n",
    "\n",
    "        output = np.zeros(grad.shape)  \n",
    "        n = grad.size\n",
    "\n",
    "        jacob = np.zeros((n,n))\n",
    "\n",
    "        for i in range(n) :\n",
    "            for j in range(n) : \n",
    "\n",
    "                jacob[i,j] *= self.stand_input[i]*self.stand_input[j]\n",
    "                jacob[i,j] *= -(self.stdev + self.epsilon)/(n*self.stdev)\n",
    "                jacob[i,j] *= +self.coeff/(n*(self.stdev + self.epsilon))\n",
    "\n",
    "\n",
    "                if i != j :\n",
    "                    \n",
    "                    \n",
    "                    jacob[i,j] += self.coeff/(n*(self.stdev + self.epsilon))\n",
    "\n",
    "                else : \n",
    "\n",
    "                    jacob[i,j] += self.coeff/(self.stdev + self.epsilon)*(1+n**(-1))\n",
    "\n",
    "\n",
    "\n",
    "        self.coeff -= np.dot(np.transpose(grad),self.stand_input)\n",
    "        self.biases -= eta*grad\n",
    "\n",
    "        return np.dot(jacob, grad)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
